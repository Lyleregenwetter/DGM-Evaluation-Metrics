{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e3cb51",
   "metadata": {},
   "source": [
    "# Paper Results Notebook\n",
    "This notebook is intended to help users replicate the results included in the paper. It also demonstrates how to set up a variaty of different types of problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389621ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:124\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookup_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m index_table_from_dataset\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookup_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table_from_dataset\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_example_dataset\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprefetching_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_to_device\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprefetching_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefetch_to_device\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\parsing_ops.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_spec\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_parsing_ops\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing_config\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,undefined-variable\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_parsing_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_config.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_ops\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_math_ops\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_math_ops.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_ops\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_ragged_math_ops\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_fn\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\python\\ops\\map_fn.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"Functional operations.\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf29\\lib\\site-packages\\tensorflow\\python\\autograph\\__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversionOptions\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Feature\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoGraphError\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m converted_call\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1012\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:673\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[1;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import importlib\n",
    "import evaluation\n",
    "import load_data\n",
    "import Padgan_variants\n",
    "import VAEs\n",
    "import utils\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(load_data)\n",
    "importlib.reload(VAEs)\n",
    "importlib.reload(Padgan_variants)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99d632",
   "metadata": {},
   "source": [
    "**General Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numgen = 1000 #Number of samples to generate\n",
    "numinst = 1 #Number of instantiations to test\n",
    "scaling = True #Scale or not\n",
    "scorebars = True #Print progress bars for scoring functions\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb84ab7",
   "metadata": {},
   "source": [
    "**Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c7e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions=[]\n",
    "\n",
    "DM_val = load_data.all_val_wrapper()\n",
    "\n",
    "#In this case, we include objectives specifically for ML efficacy\n",
    "DM_objs = [load_data.KNO1_a_wrapper(4,4), load_data.KNO1_b_wrapper(4,4)] \n",
    "\n",
    "pareto = np.stack([0.4705*np.linspace(0,1,1000), 0.4705*np.linspace(1,0,1000)], axis=1)\n",
    "sampling_func = load_data.sample_circle_blobs_wrapper(10000, 6, 1.3, 0.22) #Uniform Sampling with Number of positive samples & Negative Samples\n",
    "\n",
    "rangearr = np.array([[-2,2], [-2,2]])\n",
    "functions.append([sampling_func, DM_val, DM_objs, rangearr, None, None])\n",
    "\n",
    "reg_clf_params = None\n",
    "config_params = [False, False, False, None, None, False]\n",
    "train_params = [1, 0, 4, 5000] #Setting DPP weight to 0 for normal GAN\n",
    "DTAI_params= [None, None, None]\n",
    "\n",
    "methods=pd.Series()\n",
    "methods[\"GAN\"] = Padgan_variants.padgan_wrapper(config_params, train_params, DTAI_params, reg_clf_params, reg_clf_params)\n",
    "methods[\"VAE\"] = VAEs.VAE_wrapper([100, 128, 1e-3, 4, .05, False])\n",
    "\n",
    "metrics=pd.Series()\n",
    "metrics[\"Nearest Dataset Sample\"] = [\"minimize\", evaluation.gen_data_distance_wrapper(\"x\", \"min\")]\n",
    "metrics[\"Nearest Generated Sample\"] = [\"minimize\", evaluation.data_gen_distance_wrapper(\"x\", \"min\")]\n",
    "\n",
    "#Rediscovery needs special handling in the utils file. \n",
    "#We pass in a special flag: \"Rediscovery\" to trigger this handling. See utils file for more info\n",
    "metrics[\"Rediscovery\"] = [\"minimize\", \"Rediscovery\", evaluation.data_gen_distance_wrapper(\"x\", \"min\")]\n",
    "holdout = 0.05 #If using rediscovery, we need to hold out a portion of the data during training\n",
    "\n",
    "metrics[\"F1\"] = [\"maximize\", evaluation.F_wrapper(\"x\", 1)]\n",
    "metrics[\"F10\"] = [\"maximize\", evaluation.F_wrapper(\"x\", 10)]\n",
    "metrics[\"F0.1\"] = [\"maximize\", evaluation.F_wrapper(\"x\", 0.1)]\n",
    "metrics[\"AUC-PR\"] = [\"maximize\", evaluation.AUC_wrapper(\"x\")]\n",
    "metrics[\"MMD\"] = [\"minimize\", evaluation.MMD_wrapper('x')]\n",
    "metrics[\"ML Efficacy\"] = [\"maximize\", evaluation.ML_efficacy_wrapper(KNeighborsRegressor(n_neighbors=5), r2_score)]\n",
    "\n",
    "validity_status = 0 #whether we are considering constraints\n",
    "obj_status = 0 #whether we are considering functional performance\n",
    "conditional_status = 0 #whether we are considering conditioning\n",
    "cond_dist=False #Whether conditional metrics are compared against conditional or marginal distribution\n",
    "\n",
    "timestr = utils.fit_and_generate(functions, methods, numinst, numgen, scaling, obj_status, conditional_status, holdout)\n",
    "# timestr = \"20230212-150132\"\n",
    "utils.score(timestr, functions, methods, metrics, numinst, scaling, cond_dist, scorebars)\n",
    "utils.plot_all(timestr, functions, methods, numinst, scaling, validity_status, obj_status, conditional_status, cond_dist, \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199e19b",
   "metadata": {},
   "source": [
    "**Diversity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1892c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions=[]\n",
    "\n",
    "DM_val = load_data.all_val_wrapper()\n",
    "pareto = np.stack([0.4705*np.linspace(0,1,1000), 0.4705*np.linspace(1,0,1000)], axis=1)\n",
    "sampling_func = load_data.sample_circle_blobs_wrapper(10000, 6, 1.3, 0.22) #Uniform Sampling with Number of positive samples & Negative Samples\n",
    "\n",
    "rangearr = np.array([[-2,2], [-2,2]])\n",
    "functions.append([sampling_func, DM_val, None, rangearr, None, None])\n",
    "\n",
    "reg_clf_params = None\n",
    "config_params = [False, False, False, None, None, False]\n",
    "train_params = [1, 0, 4, 5000] #Setting DPP weight to 0 for normal GAN\n",
    "DTAI_params= [None, None, None]\n",
    "\n",
    "methods=pd.Series()\n",
    "methods[\"GAN\"] = Padgan_variants.padgan_wrapper(config_params, train_params, DTAI_params, reg_clf_params, reg_clf_params)\n",
    "methods[\"VAE\"] = VAEs.VAE_wrapper([100, 128, 1e-3, 4, .05, False])\n",
    "\n",
    "\n",
    "metrics=pd.Series()\n",
    "metrics[\"Inter-Generated Sample\"] = [\"maximize\", evaluation.gen_gen_distance_wrapper(\"x\", \"min\")]\n",
    "metrics[\"Convex Hull\"] = [\"maximize\", evaluation.convex_hull_wrapper(\"x\")]\n",
    "metrics[\"DPP Diversity\"] = [\"minimize\", evaluation.DPP_diversity_wrapper(\"x\")]\n",
    "metrics[\"Nearest Generated Sample\"] = [\"maximize\", evaluation.data_gen_distance_wrapper(\"x\", \"min\")]\n",
    "metrics[\"Distance to Centroid\"] = [\"maximize\", evaluation.distance_to_centroid_wrapper(\"x\")]\n",
    "\n",
    "\n",
    "validity_status = 0 #whether we are considering constraints\n",
    "obj_status = 0 #whether we are considering functional performance\n",
    "conditional_status = 0 #whether we are considering conditioning\n",
    "cond_dist=False #Whether conditional metrics are compared against conditional or marginal distribution\n",
    "\n",
    "timestr = utils.fit_and_generate(functions, methods, numinst, numgen, scaling, obj_status, conditional_status, holdout=0)\n",
    "\n",
    "utils.score(timestr, functions, methods, metrics, numinst, scaling, cond_dist, scorebars)\n",
    "utils.plot_all(timestr, functions, methods, numinst, scaling, validity_status, obj_status, conditional_status, cond_dist, \"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3b0bc",
   "metadata": {},
   "source": [
    "**Constraint Adherence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93767f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "functions=[]\n",
    "\n",
    "sampling_func_2 = load_data.sample_uniform_wrapper(10000, 10000) #Uniform Sampling with Number of positive samples & Negative Samples\n",
    "validity_func_2 = load_data.concentric_circles_val_wrapper(2)\n",
    "rangearr_2 = np.array([[-1,1], [-1,1]])\n",
    "functions.append([sampling_func_2, validity_func_2, None, rangearr_2, None, None])\n",
    "\n",
    "reg_clf_params = None\n",
    "config_params = [False, False, False, None, None, False]\n",
    "train_params = [1, 0, 4, 5000] #Setting DPP weight to 0 for normal GAN\n",
    "DTAI_params= [None, None, None]\n",
    "\n",
    "methods=pd.Series()\n",
    "methods[\"GAN\"] = Padgan_variants.padgan_wrapper(config_params, train_params, DTAI_params, reg_clf_params, reg_clf_params)\n",
    "methods[\"VAE\"] = VAEs.VAE_wrapper([100, 128, 1e-3, 4, .05, False])\n",
    "\n",
    "\n",
    "metrics=pd.Series()\n",
    "metrics[\"Predicted Constraint Satisfaction\"] = [\"maximize\", evaluation.predicted_constraint_satisfaction_wrapper(KNeighborsClassifier(n_neighbors=5))]\n",
    "metrics[\"Validity\"] = [\"maximize\", \"Validity\"] #Validity is handled specially in utils\n",
    "metrics[\"Nearest Invalid Sample\"] = [\"maximize\", evaluation.gen_neg_distance_wrapper(\"min\")]\n",
    "\n",
    "\n",
    "validity_status = 1 #whether we are considering constraints\n",
    "obj_status = 0 #whether we are considering functional performance\n",
    "conditional_status = 0 #whether we are considering conditioning\n",
    "cond_dist=False #Whether conditional metrics are compared against conditional or marginal distribution\n",
    "\n",
    "timestr = utils.fit_and_generate(functions, methods, numinst, numgen, scaling, obj_status, conditional_status, holdout=0)\n",
    "\n",
    "utils.score(timestr, functions, methods, metrics, numinst, scaling, cond_dist, scorebars)\n",
    "utils.plot_all(timestr, functions, methods, numinst, scaling, validity_status, obj_status, conditional_status, cond_dist, \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbccec",
   "metadata": {},
   "source": [
    "**Performance and Target Achievement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac932eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions=[]\n",
    "\n",
    "Perf_val = load_data.all_val_wrapper()\n",
    "pareto = np.stack([0.4705*np.linspace(0,1,1000), 0.4705*np.linspace(1,0,1000)], axis=1)\n",
    "sampling_func = load_data.sample_circle_blobs_wrapper(10000, 6, 1.3, 0.22) #Uniform Sampling with Number of positive samples & Negative Samples\n",
    "Perf_objs = [load_data.KNO1_a_wrapper(4,4), load_data.KNO1_b_wrapper(4,4)] \n",
    "rangearr = np.array([[-2,2], [-2,2]])\n",
    "functions.append([sampling_func, Perf_val, Perf_objs, rangearr, None, None])\n",
    "\n",
    "reg_clf_params = None\n",
    "config_params = [False, False, False, None, None, False]\n",
    "train_params = [1, 0, 4, 5000] #Setting DPP weight to 0 for normal GAN\n",
    "DTAI_params= [None, None, None]\n",
    "\n",
    "methods=pd.Series()\n",
    "methods[\"GAN\"] = Padgan_variants.padgan_wrapper(config_params, train_params, DTAI_params, reg_clf_params, reg_clf_params)\n",
    "\n",
    "# Regressor/Classifier params: [dropout, layers, layersize, batchnorm, activation, patience, lr, batchsize, epochs]\n",
    "reg_clf_params = [0.1, 2, 100, True, \"Leaky ReLU\", 30, 1e-4, 32, 100]\n",
    "config_params = [False, False, False, \"auto\", \"auto\", False]\n",
    "train_params = [5, 2, 4, 5000]\n",
    "DTAI_params= [\"auto\", \"auto\", \"auto\"]\n",
    "methods[\"MO-PaDGAN\"] = Padgan_variants.padgan_wrapper(config_params, train_params, DTAI_params, reg_clf_params, reg_clf_params)\n",
    "\n",
    "metrics=pd.Series()\n",
    "target = np.array([0.5, 0.5])\n",
    "a_ = np.array([1, 1])\n",
    "p_ = np.array([1, 1])\n",
    "direction = \"maximize\" #Whether to maximize or minimize the objective function. Not to be confused with whether evaluation metrics are best maximized or minimized!\n",
    "metrics[\"DTAI\"] = [\"maximize\", evaluation.DTAI_wrapper(direction, target, a_, p_)]\n",
    "metrics[\"Hypervolume\"] = [\"maximize\", evaluation.Hypervolume_wrapper()]\n",
    "metrics[\"Generational Distance\"] = [\"minimize\", evaluation.Generational_distance_wrapper(pareto)]\n",
    "metrics[\"Weighted Target Success Rate\"] = [\"maximize\", evaluation.weighted_target_success_rate_wrapper(direction, target, p_)]\n",
    "metrics[\"Signed Distance to Target\"] = [\"maximize\", evaluation.signed_distance_to_boundary_wrapper(direction, target, a_)]\n",
    "\n",
    "\n",
    "validity_status = 0 #whether we are considering constraints\n",
    "obj_status = 1 #whether we are considering functional performance\n",
    "conditional_status = 0 #whether we are considering conditioning\n",
    "cond_dist=False #Whether conditional metrics are compared against conditional or marginal distribution\n",
    "plotobjs = target\n",
    "\n",
    "timestr = utils.fit_and_generate(functions, methods, numinst, numgen, scaling, obj_status, conditional_status, holdout=0)\n",
    "\n",
    "utils.score(timestr, functions, methods, metrics, numinst, scaling, cond_dist, scorebars)\n",
    "utils.plot_all(timestr, functions, methods, numinst, scaling, validity_status, obj_status, conditional_status, cond_dist, \"purple\", plotobjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0c8ff",
   "metadata": {},
   "source": [
    "**Conditioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53120af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Note: only continuous conditioning is currently supported\n",
    "\n",
    "importlib.reload(load_data)\n",
    "# importlib.reload(GANs)\n",
    "importlib.reload(VAEs)\n",
    "\n",
    "functions=[]\n",
    "\n",
    "DM_val = load_data.all_val_wrapper()\n",
    "pareto = np.stack([0.4705*np.linspace(0,1,1000), 0.4705*np.linspace(1,0,1000)], axis=1)\n",
    "sampling_func = load_data.sample_circle_blobs_wrapper(10000, 6, 1.3, 0.22) #Uniform Sampling with Number of positive samples & Negative Samples\n",
    "rangearr = np.array([[-2,2], [-2,2]])\n",
    "cond_func = load_data.exp_obj_wrapper(1,1)\n",
    "cond=0.3\n",
    "functions.append([sampling_func, DM_val, None, rangearr, cond_func, cond])\n",
    "\n",
    "\n",
    "reg_clf_params = None\n",
    "config_params_cond = [False, False, False, None, None, True]\n",
    "train_params = [1, 0, 4, 5000] #Setting DPP weight to 0 for normal GAN\n",
    "DTAI_params= [None, None, None]\n",
    "\n",
    "methods=pd.Series()\n",
    "methods[\"cGAN\"] = Padgan_variants.padgan_wrapper(config_params_cond, train_params, DTAI_params, reg_clf_params, reg_clf_params)\n",
    "methods[\"cVAE\"] = VAEs.VAE_wrapper([100, 128, 1e-3, 4, .05, True])\n",
    "\n",
    "metrics=pd.Series()\n",
    "metrics[\"Nearest Dataset Sample\"] = [\"minimize\", evaluation.gen_data_distance_wrapper(\"x\", \"min\")]\n",
    "metrics[\"Nearest Generated Sample\"] = [\"minimize\", evaluation.data_gen_distance_wrapper(\"x\", \"min\")]\n",
    "metrics[\"F1\"] = [\"maximize\", evaluation.F_wrapper(\"x\", 1)]\n",
    "metrics[\"F10\"] = [\"maximize\", evaluation.F_wrapper(\"x\", 10)]\n",
    "metrics[\"F0.1\"] = [\"maximize\", evaluation.F_wrapper(\"x\", 0.1)]\n",
    "metrics[\"AUC-PR\"] = [\"maximize\", evaluation.AUC_wrapper(\"x\")]\n",
    "metrics[\"MMD\"] = [\"minimize\", evaluation.MMD_wrapper('x')]\n",
    "\n",
    "#Conditioning Reconstruction and Adherence needs special handling in the utils file. We pass in their respective flags as follows:\n",
    "metrics[\"Conditioning Reconstruction\"] = [\"maximize\", \"Conditioning Reconstruction\", evaluation.predicted_conditioning_wrapper(KNeighborsRegressor(n_neighbors=5), cond)]\n",
    "metrics[\"Conditioning Adherence\"] = [\"maximize\", \"Conditioning Adherence\"]\n",
    "\n",
    "validity_status = 0 #whether we are considering constraints\n",
    "obj_status = 0 #whether we are considering functional performance\n",
    "conditional_status = 1 #whether we are considering conditioning\n",
    "cond_dist=True #Whether conditional metrics are compared against conditional or marginal distribution\n",
    "\n",
    "\n",
    "#Generate a new set of results\n",
    "timestr = utils.fit_and_generate(functions, methods, numinst, numgen, scaling, obj_status, conditional_status, holdout=0)\n",
    "\n",
    "#OR Load a set of results from a timestring:\n",
    "# timestr= \"20230204-161902\"\n",
    "\n",
    "plotobjs = [cond]\n",
    "\n",
    "utils.score(timestr, functions, methods, metrics, numinst, scaling, cond_dist, scorebars)\n",
    "utils.plot_all(timestr, functions, methods, numinst, scaling, validity_status, obj_status, conditional_status, cond_dist, \"orange\", plotobjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89000235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef481bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
